title:: Activation Functions


<figure style="padding: 1em;">
<img src="https://storage.googleapis.com/kaggle-media/learn/images/OLSUEYT.png" width="400" alt=" ">
<figcaption style="textalign: center; font-style: italic"><center>Without activation functions, neural networks can only learn linear relationships. In order to fit curves, we'll need to use activation functions. 
</center></figcaption>
</figure>

An activation function is simply some function we apply to each of a layer's outputs (its **activation**). the most common activation function is the _rectifier_ function $max(0, x)$.

<figure style="padding: 1em;">
<img src="https://storage.googleapis.com/kaggle-media/learn/images/aeIyAlF.png" width="400" alt="A graph of the rectifier function. The line y=x when x>0 and y=0 when x<0, making a 'hinge' shape like '_/'.">
<figcaption style="textalign: center; font-style: italic"><center>
</center></figcaption>
</figure>

## Rectifier function

The rectifier function has a graph that's a line with the negative part "rectified" to zero. Applying the function to the outputs of a neuron will aad a _bend_ in the data, moving us away from simple lines.

when we attach the rectifier to a [linear unit](../linear-unit/index.qmd), we get a **Re**ctified **L**inear **Un**it or **ReLU**. Applying a ReLU activation function to a linear unit means the output becomes $max(0, w \dot x + b)$ which we might draw in a diagram like:

<figure style="padding: 1em;">
<img src="https://storage.googleapis.com/kaggle-media/learn/images/eFry7Yu.png" width="250" alt="Diagram of a single ReLU. Like a linear unit, but instead of a '+' symbol we now have a hinge '_/'. ">
<figcaption style="textalign: center; font-style: italic"><center>A rectified linear unit.
</center></figcaption>
</figure>
