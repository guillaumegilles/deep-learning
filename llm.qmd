---
title: large Language Models
---
Think of LLM as mostly incrustable artifacts

1. Model training

1.1 Pretraining 
needs big chunk of the internet: ~10TB of text > 6,000 GPUs cluster for 12 days (~ 2M$) ie: 1e24 FLOPS > 140GB files
- crawl of internet text
= basicall compress ~ 10TB internet texts into a 140GB zip file = lossely compress into llm parameters (=/ lossless). 
=> base model
~ iterate every year

1.2 finetuning
  1. write labeling instructions
  2. Hire people to collect 100k high quality q&a repsonses and/or comparisons (scale.ai to automate)
  3. finetune base model on these data
  4. obtain an assistant
  5. run evaluation
  6. deploy
  7. monitor, collect misbehavior, go back to step 1
swap the pretrained dataset of internet text in favor of curated set of document written by people, think of it of quality Q&A discussion.
=> get an assistant after finetuning
= somehow, llm emprirically understands, after the finetuning stage, to behave like a helpfuil assistant because it "saw" hundred of thousand Q&A hight quality examples.
~ iterate every week

1.3 Reinforcement Learning From Human - RLFH
It is often much easier, for humans, to compare LLM answers rather than writing an correct answer. For example, prompted the question "Write a haiku about windows" I am personnaly more confortable evaluating 3 propose answer from an LLM instead of writing an haiku myself.
https://openai.com/index/instruction-following/ = https://arxiv.org/pdf/2203.02155


2. Model **inference**
llama-2-70b :
- parameters = 140 GB car chaque paramètres stores into 2 bytes (float 16 numbers data type)
- run.c (~500 lines of code with no dependencies) that runs the parameters (could be a pyhton file as well)

## LLM Scaling Law

https://arxiv.org/abs/2203.15556
https://arxiv.org/abs/2303.12712

LLM's performance is a smooth and predictable function of:
 - $N$, numbers of neural network's parameters
 - $D$, amount of text trained on

=> we can expect more "intelligence" for free by scaling
=> gold rush on more computational power and dataset size = better performance

https://www.youtube.com/watch?v=zjkBMFhNj_g

## Reasoning approch

based on the book thinking fast é thinking slow, human reasoning is 2 stage. The first one is fast and instinctive, while the other one is slower but deeper and more computationaly intensive. 
Initially LLM only work with system 1. = paper: https://arxiv.org/abs/2305.10601 + https://arxiv.org/abs/2305.08291 implemented Tree-of-Thought (ToT) prompt engineering paradigm.
=> we want LLM to think: sacrifice time to accuracy

## Rise of Agentic Assistants

multimodal llm

LLM uses computer tools, such as web search, programming language, DALL-E, etc. to enhance and improve their performance.
+ speech, image genration, image recognition

=> LLM OS approach: LLM as a kernel of a "new type" of operating system, agentic one, self autonomous 

## Problems of LLM

- hallucination

## LLM Security

https://owasp.org/www-project-top-10-for-large-language-model-applications/

### Jailbreak

https://arxiv.org/abs/2307.02483
Roleplayed
popsup safety measure implemented by companies
https://arxiv.org/abs/2307.15043
https://arxiv.org/abs/2306.13213

### Prompt Injection

https://arxiv.org/abs/2302.12173
https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/

### Data Poisoning / Backdoor Attack

https://arxiv.org/abs/2305.00944
https://arxiv.org/abs/2302.10149
